## 이차원 이산형 확률변수
2개의 불공정한 주사위를 사용한다고 가정

## 1. 2차원 이산형 확률변수의 정의


### 1.1 결합확률분포 (joint probability distribution)
1차원 확률분포 X, Y를 동시에 다루며 이둘이 취할 수 있는 조합으로 이루어진 집합과 그 확률분포를 결합확률분포라고함


- $P(X=x, Y=y) = f_{xy}(x,y)$ 

### 1.2 확률의 성질
1) 2차원 이산형 확률도 1차원의 경우와 마찬가지로 확률은 0이상
2) 전체 확률은 1이 되어야한다

- $f_{xy}(x_i, y_i) >= 0$
- $\sum_i\sum_jf_{xy}(x_i,y_i) = 1$


``` python
x_set = np.arange(2, 13)
y_set = np.arange(1, 7)

def f_XY(x, y): #확률정의
    if 1 <= y <=6 and 1 <= x - y <= 6:
        return y * (x-y) / 441
    else:
        return 0

XY = [x_set, y_set, f_XY]

prob = np.array([[f_XY(x_i, y_j) for y_j in y_set]
                 for x_i in x_set])

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111)

c = ax.pcolor(prob)
ax.set_xticks(np.arange(prob.shape[1]) + 0.5, minor=False)
ax.set_yticks(np.arange(prob.shape[0]) + 0.5, minor=False)
ax.set_xticklabels(np.arange(1, 7), minor=False)
ax.set_yticklabels(np.arange(2, 13), minor=False)

# y축을 내림차순의 숫자가 되게 하여, 위 아래를 역전시킨다
ax.invert_yaxis()

# x축의 눈금을 그래프 위쪽에 표시
ax.xaxis.tick_top()
fig.colorbar(c, ax=ax)
plt.show()
```

``` python
# 확률성질 체크
np.all(prob >= 0)
np.sum(prob)
```


### 1.3 주변확률분포 (marginal probability distribution)
- 결합확률분포를 사용할 경우 동시에 확률변수 2개를 고려하게 됨
- 따라서 개별 확률분포가 궁금할 경우 주변 확률분포를 사용

**확률변수 X의 확률함수 $F_X(x)$ 는 결합함수 $f_{XY}$ 에서 Y가 취할 수 있는 값 모두를 대입한 다음 모두 더한 값으로 구할 수 있다**


- $\sum_k f_{XY}(x, y_k)$  --> 이를 $f_X(x)$를 X의 주변확률분포라고 함
- 이렇게 결합확률함수 $f_{XY}$ 에서 Y값의 영향력을 제거한다고 보면 쉬움


``` python
# 여기서 f_XY함수는 y*(x-y)/441
def f_X(x):
    return np.sum([f_XY(x, y_k) for y_k in y_set])

def f_Y(y):
    return np.sum([f_XY(x_k, y) for x_k in x_set])

X = [x_set, f_X]
Y = [y_set, f_Y]

prob_x = np.array([f_X(x_k) for x_k in x_set])
prob_y = np.array([f_Y(y_k) for y_k in y_set])

fig = plt.figure(figsize=(12, 4))
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)

ax1.bar(x_set, prob_x)
ax1.set_title('X_marginal probability distribution')
ax1.set_xlabel('X_value')
ax1.set_ylabel('probability')
ax1.set_xticks(x_set)

ax2.bar(y_set, prob_y)
ax2.set_title('Y_marginal probability distribution')
ax2.set_xlabel('Y_value')
ax2.set_ylabel('probability')

plt.show()
```


> 결과


<img width="765" alt="스크린샷 2023-08-06 오후 8 48 53" src="https://github.com/hozyhozy/Statistics/assets/123252821/28b617ff-4383-47be-ae73-023901f617cc">


## 2. 2차원 이산형 확률변수의 지표

### 2.1 기댓값
$x_i$ 와 확률의 곱으로 표현 가능

- $\mu_{X} = E(X) = \sum_i\sum_j x_i * f_{XY}(x_i, y_j)$
- 기댓값의 선형성 : $E(aX+bY) = aE(X)+bE(Y)$


``` python
np.sum([x_i * f_XY(x_i, y_j) for x_i in x_set for y_j in y_set])

#확률변수 변환버전
def E(XY, g):
    x_set, y_set, f_XY = XY
    return np.sum([g(x_i, y_j) * f_XY(x_i, y_j)
                   for x_i in x_set for y_j in y_set])

mean_X = E(XY, lambda x, y: x)
mean_Y = E(XY, lambda x, y: y)

#선형성

a, b = 2, 3
E(XY, lambda x, y: a*x + b*y)
a * mean_X + b * mean_Y
```

### 2.2 분산
X에 대한 편차 제곱의 기댓값으로 표현 가능

- $\sigma^2_X = V(X) = \sum_i\sum_j(x_i - \mu{x})^2* f_{XY}(x_i, y_j)$

``` python
np.sum([(x_i-mean_X)**2 * f_XY(x_i, y_j) for x_i in  _set for y_j in y_set])
```

g(X, Y)의 분산을 구한다면?


- $\sum_i\sum_j(g(x_i, y_j) - E(g(X, Y)))^2 * f_{XY}(x_i, y_j)$


```python
def V(XY, g):
    x_set, y_set, f_XY = XY
    mean = E(XY, g)
    return np.sum([(g(x_i, y_j) - mean) ** 2 * f_XY(x_i, y_j) for x_i in x_set for y_j in y_set])
```

``` python
var_X=V(XY, g=lambda x, y:x)
var_Y=V(XY, g=lambda x, y:y)
```

### 2.3 공분산

공분산은 서로 다른 변수들 사이에 얼마나 의존하는지를 수치적으로 표현하며, 그것의 직관적 의미는 어떤 변수(X)가 평균으로부터 증가 또는 감소라는 경향을 보일 때, 이러한 경향을 다른 변수(Y 또는 Z 등등)가 따라 하는 정도를 수치화 한 것이다. (공분산은 또한 두 변수의 선형 관계의 관련성을 측정한다라고도 할 수 있다.)


*선형관계란, X가 증가(감소)할때 Y도 증가(감소)하는 것 즉, 의존성*


Corr(X,Y)는 -1부터 1까지의 값을 가지며, -1일 때는 가장 Negative 의존성이 높은 상태이며, +1이면 가장 Positive 의존성이 높은 상태이고 0일 때는 의존성이 없는 상태이다. 


하지만, 공분산이 0이라고 해서 항상 독립이 되진 않는다. 이유는 공분산은 선형 관계에 대한 의존성을 말하고, 공분산이 0이 되면 두 변수의 선형 관계가 없다는 것을 의미하지, 모든 관계가 없다는 것을 의미하진 않기 때문이다. 독립이라는 조건은 두 변수의 모든 관계가 없다는 것을 의미하므로, 그냥 독립이라는 개념이 보다 상위 개념이 된다.


[출처] Covariance (COV: 공분산)란?|작성자 PN


- $\sigma_{XY} = Cov(X, Y) = \sum_i\sum_j(x_i - \mu{x})(y_i - \mu{y})* f_{XY}(x_i, y_j)$


*------공분산이 0 일때 즉, X, Y 서로 관계가 없을 경우를 식으로 확인해본다면------*


- X, Y를 각각 독립이다.즉, E(XY) = E(X) * E(Y) 라는 의미
- X, Y의 평균을 0으로 centering했을 경우 $Cov(X, Y) = E(XY) - \mu{x}\mu{y}$ 즉, 0이 된다.
  
*----------------------------------------------------------------*

``` python
def Cov(X, Y):
    x_set, y_set, f_XY = XY
    mean_X = E(XY, lambda x, y:x)
    mean_Y = E(XY, lambda x, y:y)
    return np.sum([(x_i-mean_X) * (y_i - mean_Y) * f_XY(x_i, y_j) for x_i in x_set for y_j in y_set])

cov_xy = Cov(XY)
```


### 2.4 공분산과 분산의 관계

- $V(aX+bY) = a^2V(X) + b^2V(Y) = a^2V(X) + b^2V(Y) + 2abCov(X,Y)$


``` python
V(XY, lambda x, y: a*x + b*y)

a**2 * var_X + b**2 + var_Y + 2*a*b *cov_xy
```


### 2.5 상관계수
공분산을 표준편차로 나눈것

- $p_{xy} =  P(X, Y) =  \sigma_{XY} / \sigma_{X} \sigma_{Y}$


``` python
cov_xy / np.sqrt(var_X, var_Y)
```
